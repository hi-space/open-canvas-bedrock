"""
Utility functions for agents.
"""
from typing import Optional, Dict, Any, List
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
import uuid
import base64
import os


def format_reflections(
    reflections: Dict[str, Any],
    only_style: bool = False,
    only_content: bool = False
) -> str:
    """Format reflections for use in prompts."""
    if only_style and only_content:
        raise ValueError("Cannot specify both only_style and only_content as true.")

    style_rules = reflections.get("styleRules", [])
    if not isinstance(style_rules, list):
        try:
            import json
            style_rules = json.loads(style_rules) if isinstance(style_rules, str) else []
        except:
            style_rules = []
    
    style_str = "\n- ".join(style_rules) if style_rules else "No style guidelines found."
    
    content_rules = reflections.get("content", [])
    if not isinstance(content_rules, list):
        try:
            import json
            content_rules = json.loads(content_rules) if isinstance(content_rules, str) else []
        except:
            content_rules = []
    
    content_str = "\n- ".join(content_rules) if content_rules else "No memories/facts found."

    style_string = f"""The following is a list of style guidelines previously generated by you:
<style-guidelines>
- {style_str}
</style-guidelines>"""
    
    content_string = f"""The following is a list of memories/facts you previously generated about the user:
<user-facts>
- {content_str}
</user-facts>"""

    if only_style:
        return style_string
    if only_content:
        return content_string

    return style_string + "\n\n" + content_string


def get_model_config(
    config: RunnableConfig,
    is_tool_calling: bool = False
) -> Dict[str, Any]:
    """Get model configuration from config, supporting only AWS Bedrock."""
    from core.models import DEFAULT_MODEL_NAME
    
    configurable = config.get("configurable", {}) if config else {}
    custom_model_name = configurable.get("customModelName")
    
    # Debug logging
    if not custom_model_name:
        print(f"WARNING: customModelName not found in config. Configurable keys: {list(configurable.keys())}")
        print(f"WARNING: Full config: {config}")
        # Use default model name as fallback
        custom_model_name = DEFAULT_MODEL_NAME
        print(f"WARNING: Using default model name: {custom_model_name}")

    model_config = configurable.get("modelConfig", {})
    
    # Remove bedrock/ prefix if present (for compatibility with frontend)
    # The actual Bedrock model ID doesn't need the prefix
    if custom_model_name.startswith("bedrock/"):
        actual_model_name = custom_model_name.replace("bedrock/", "", 1)
    else:
        actual_model_name = custom_model_name
    
    return {
        "modelName": actual_model_name,
        "modelProvider": "bedrock",
        "modelConfig": model_config,
        "region": os.getenv("AWS_DEFAULT_REGION", "us-east-1"),
        "credentials": {
            "aws_access_key_id": os.getenv("AWS_ACCESS_KEY_ID"),
            "aws_secret_access_key": os.getenv("AWS_SECRET_ACCESS_KEY"),
        }
    }


def format_messages(messages: List[BaseMessage], max_length: Optional[int] = None) -> str:
    """Format messages for display.
    
    Args:
        messages: List of messages to format
        max_length: Maximum length of formatted string. If exceeded, truncate from the beginning.
    """
    formatted = []
    for idx, msg in enumerate(messages):
        msg_type = msg.__class__.__name__
        content = msg.content if isinstance(msg.content, str) else str(msg.content)
        formatted.append(f'<{msg_type} index="{idx}">\n{content}\n</{msg_type}>')
    
    result = "\n".join(formatted)
    
    if max_length and len(result) > max_length:
        # Truncate from the beginning, keeping the most recent messages
        truncated = result[-max_length:]
        # Try to find a message boundary
        first_msg_start = truncated.find('<')
        if first_msg_start > 0:
            truncated = truncated[first_msg_start:]
        return f"[Previous conversation truncated due to length...]\n{truncated}"
    
    return result


def estimate_input_size(content: str) -> int:
    """Estimate the size of content in tokens/characters.
    Rough estimate: 1 token â‰ˆ 4 characters for English text.
    For base64 images, use actual character count.
    """
    return len(content)


def truncate_content(content: str, max_size: int, suffix: str = "...[truncated]") -> str:
    """Truncate content to maximum size, preserving the end."""
    if len(content) <= max_size:
        return content
    
    # Keep the end of the content
    truncated = content[-(max_size - len(suffix)):]
    return suffix + truncated


def create_ai_message_from_web_results(web_results: List[Dict[str, Any]]) -> AIMessage:
    """Create an AI message from web search results."""
    web_results_str = "\n\n".join([
        f'''<search-result
      index="{index}"
      publishedDate="{r.get('metadata', {}).get('publishedDate', 'Unknown')}"
      author="{r.get('metadata', {}).get('author', 'Unknown')}"
    >
      [{r.get('metadata', {}).get('title', 'Unknown title')}]({r.get('metadata', {}).get('url', 'Unknown URL')})
      {r.get('pageContent', '')}
    </search-result>'''
        for index, r in enumerate(web_results)
    ])

    content = f"Here is some additional context I found from searching the web. This may be useful:\n\n{web_results_str}"

    return AIMessage(
        content=content,
        id=f"web-search-results-{uuid.uuid4()}",
        additional_kwargs={
            "webSearchResults": web_results,
            "webSearchStatus": "done",
        }
    )


def get_string_from_content(content: Any) -> str:
    """Extract string from message content."""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        return "\n".join([
            item.get("text", "") if isinstance(item, dict) and "text" in item else str(item)
            for item in content
        ])
    return str(content)


def clean_base64(base64_string: str) -> str:
    """Clean base64 string by removing data URL prefix and fixing padding."""
    import re
    
    # Remove data URL prefix if present (e.g., "data:application/pdf;base64,")
    cleaned = re.sub(r'^data:[^;]+;base64,', '', base64_string)
    
    # Remove whitespace
    cleaned = cleaned.replace('\n', '').replace('\r', '').replace(' ', '')
    
    # Fix padding: base64 strings must be a multiple of 4
    # Add padding if necessary
    missing_padding = len(cleaned) % 4
    if missing_padding:
        cleaned += '=' * (4 - missing_padding)
    
    return cleaned


# Artifact utility functions
def is_artifact_code_content(content: Any) -> bool:
    """Check if artifact content is code type."""
    return (
        isinstance(content, dict) and
        content.get("type") == "code"
    )


def is_artifact_markdown_content(content: Any) -> bool:
    """Check if artifact content is markdown/text type."""
    return (
        isinstance(content, dict) and
        content.get("type") == "text"
    )


def get_artifact_content(artifact: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """Get current artifact content from artifact."""
    if not artifact:
        return None
    
    if not isinstance(artifact, dict):
        return None
    
    contents = artifact.get("contents", [])
    if not contents:
        return None
    
    current_index = artifact.get("currentIndex")
    if current_index:
        # Find content with matching index
        for content in contents:
            if isinstance(content, dict) and content.get("index") == current_index:
                return content
    
    # Return last content if no current index
    return contents[-1] if contents else None


def format_artifact_content(
    content: Dict[str, Any],
    shorten_content: bool = False
) -> str:
    """Format artifact content for display."""
    if not content:
        return ""
    
    title = content.get("title", "Untitled")
    artifact_type = content.get("type", "unknown")
    
    if is_artifact_code_content(content):
        code = content.get("code", "")
        artifact_content = code[:500] if shorten_content else code
    else:
        markdown = content.get("fullMarkdown", "")
        artifact_content = markdown[:500] if shorten_content else markdown
    
    return f"Title: {title}\nArtifact type: {artifact_type}\nContent: {artifact_content}"


def format_artifact_content_with_template(
    template: str,
    content: Dict[str, Any],
    shorten_content: bool = False
) -> str:
    """Format artifact content with template."""
    formatted = format_artifact_content(content, shorten_content)
    return template.replace("{artifact}", formatted)


# Thinking model utility functions
def is_thinking_model(model_name: str) -> bool:
    """Check if model is a thinking model (o1, o3, etc.)."""
    thinking_models = ["o1", "o3", "o1-mini", "o3-mini"]
    return any(model in model_name.lower() for model in thinking_models)


def extract_thinking_and_response_tokens(text: str) -> Dict[str, str]:
    """Extract thinking and response tokens from text with <think> tags."""
    think_start_tag = "<think>"
    think_end_tag = "</think>"
    
    start_index = text.find(think_start_tag)
    
    # No thinking tag found
    if start_index == -1:
        return {
            "thinking": "",
            "response": text.strip(),
        }
    
    after_start_tag = text[start_index + len(think_start_tag):]
    end_index = after_start_tag.find(think_end_tag)
    
    # If no closing tag, all remaining text is thinking
    if end_index == -1:
        return {
            "thinking": after_start_tag.strip(),
            "response": text[:start_index].strip(),
        }
    
    # We have both opening and closing tags
    thinking = after_start_tag[:end_index].strip()
    response = (
        text[:start_index] +
        after_start_tag[end_index + len(think_end_tag):]
    ).strip()
    
    return {
        "thinking": thinking,
        "response": response,
    }


def get_formatted_reflections(config: RunnableConfig) -> str:
    """Get formatted reflections from config or store."""
    if not config:
        return "No reflections found."
    
    configurable = config.get("configurable", {})
    reflections_dict = configurable.get("reflections", {})
    
    # If not in config, try to get from store
    if not reflections_dict:
        assistant_id = configurable.get("open_canvas_assistant_id")
        if assistant_id:
            try:
                from store.store import store
                namespace = ["memories", assistant_id]
                key = "reflection"
                store_item = store.get_item(namespace, key)
                if store_item and store_item.get("value"):
                    reflections_dict = store_item["value"]
            except Exception:
                # If store access fails, continue with empty reflections
                pass
    
    if not reflections_dict:
        return "No reflections found."
    
    return format_reflections(reflections_dict)


def extract_urls(text: str) -> List[str]:
    """Extract all URLs from a given string."""
    import re
    urls = set()
    
    # Match markdown links: [text](url)
    markdown_link_pattern = r'\[([^\]]+)\]\((https?://[^\s)]+)\)'
    for match in re.finditer(markdown_link_pattern, text):
        urls.add(match.group(2))
        # Replace with spaces to avoid double-matching
        text = text.replace(match.group(0), " " * len(match.group(0)))
    
    # Match plain URLs
    plain_url_pattern = r'https?://[^\s<\]]+(?:[^<.,:;"\'\]\s)]|(?=\s|$))'
    for match in re.finditer(plain_url_pattern, text):
        urls.add(match.group(0))
    
    return list(urls)


def convert_pdf_to_text(base64_pdf: str) -> str:
    """Convert base64-encoded PDF to text."""
    try:
        import PyPDF2
        import io
        
        # Clean the base64 input
        cleaned_base64 = clean_base64(base64_pdf)
        
        # Convert to bytes
        pdf_bytes = base64.b64decode(cleaned_base64)
        
        # Parse PDF
        pdf_file = io.BytesIO(pdf_bytes)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # Extract text from all pages
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        
        return text.strip()
    except Exception as e:
        print(f"Error converting PDF to text: {e}", flush=True)
        # Fallback: try pdf-parse if available
        try:
            import subprocess
            import tempfile
            import os
            
            cleaned_base64 = clean_base64(base64_pdf)
            pdf_bytes = base64.b64decode(cleaned_base64)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(pdf_bytes)
                tmp_path = tmp_file.name
            
            try:
                # Try using pdf-parse via node if available
                result = subprocess.run(
                    ['node', '-e', 
                     f'const pdfParse = require("pdf-parse"); const fs = require("fs"); '
                     f'const dataBuffer = fs.readFileSync("{tmp_path}"); '
                     f'pdfParse(dataBuffer).then(data => console.log(data.text));'],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if result.returncode == 0:
                    return result.stdout.strip()
            finally:
                os.unlink(tmp_path)
            
            raise e
        except:
            raise e


def create_context_document_messages(
    config: RunnableConfig,
    context_documents: Optional[List[Dict[str, Any]]] = None
) -> List[Dict[str, Any]]:
    """Create context document messages from documents.
    
    For Bedrock, we'll convert PDFs to text and include as text content.
    """
    from langchain_core.messages import HumanMessage
    
    # Get documents from config if not provided
    if context_documents is None:
        configurable = config.get("configurable", {}) if config else {}
        context_documents = configurable.get("contextDocuments", [])
    
    if not context_documents:
        print("create_context_document_messages: No documents provided", flush=True)
        return []
    
    print(f"create_context_document_messages: Processing {len(context_documents)} documents", flush=True)
    
    messages = []
    for doc in context_documents:
        doc_type = doc.get("type", "")
        doc_data = doc.get("data", "")
        doc_name = doc.get("name", "Unknown document")
        
        # Debug logging
        print(f"Processing document: name={doc_name}, type={doc_type}, data_length={len(doc_data) if doc_data else 0}", flush=True)
        
        if not doc_data:
            print(f"Warning: Document '{doc_name}' has no data, skipping", flush=True)
            continue
        
        if doc_type == "application/pdf":
            # Convert PDF to text
            try:
                text = convert_pdf_to_text(doc_data)
                # Include document name so LLM knows which document it is
                formatted_text = f"File: {doc_name}\n\n{text}"
                messages.append({
                    "type": "text",
                    "text": formatted_text
                })
            except Exception as e:
                print(f"Failed to convert PDF: {e}", flush=True)
                # Skip this document
                continue
        elif doc_type.startswith("text/"):
            # Decode base64 text
            try:
                cleaned = clean_base64(doc_data)
                text = base64.b64decode(cleaned).decode('utf-8')
                # Include document name so LLM knows which document it is
                formatted_text = f"File: {doc_name}\n\n{text}"
                messages.append({
                    "type": "text",
                    "text": formatted_text
                })
            except Exception as e:
                print(f"Failed to decode text document: {e}", flush=True)
                continue
        elif doc_type == "text":
            # Plain text
            # Include document name so LLM knows which document it is
            formatted_text = f"File: {doc_name}\n\n{doc_data}"
            messages.append({
                "type": "text",
                "text": formatted_text
            })
        elif doc_type.startswith("image/"):
            # Handle images - pass as image_url format for LLM
            # Check image size to avoid "Input is too long" errors
            # Base64 encoding increases size by ~33%, so we check the base64 string length
            # Maximum recommended: ~2MB base64 (allows ~1.5MB original image)
            # Reduced to avoid "Input is too long" errors and improve efficiency
            # Base64 encoding is inefficient (~33% overhead), so we keep images small
            MAX_IMAGE_BASE64_SIZE = 2 * 1024 * 1024  # 2MB
            
            if len(doc_data) > MAX_IMAGE_BASE64_SIZE:
                print(f"Warning: Image '{doc_name}' is too large ({len(doc_data)} bytes). "
                      f"Maximum recommended size is {MAX_IMAGE_BASE64_SIZE} bytes. "
                      f"Skipping image to avoid model input length errors.", flush=True)
                # Include a text message indicating the image was too large
                messages.append({
                    "type": "text",
                    "text": f"File: {doc_name} (image - too large, skipped. Please compress the image to under 5MB base64 size)"
                })
                continue
            
            # doc_data should already be in base64 format (data:image/...;base64,...)
            # If it's just base64 without prefix, add the data URL prefix
            if not doc_data.startswith("data:"):
                # Clean base64 string
                cleaned = clean_base64(doc_data)
                # Create data URL
                image_data_url = f"data:{doc_type};base64,{cleaned}"
            else:
                image_data_url = doc_data
            
            # Include document name as text, then the image
            messages.append({
                "type": "text",
                "text": f"File: {doc_name} (image)"
            })
            messages.append({
                "type": "image_url",
                "image_url": {
                    "url": image_data_url
                }
            })
    
    if not messages:
        return []
    
    # Return as a single user message with context
    return [{
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Use the file(s) and/or text below as context when generating your response."
            },
            *messages
        ]
    }]

